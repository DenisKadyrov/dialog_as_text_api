import json
import wave
import os
import librosa

import numpy as np

from fastapi import UploadFile
from pydub import AudioSegment
from vosk import Model, KaldiRecognizer

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout


def create_model(vector_length: int=128):
    """5 hidden dense layers from 256 units to 64, not the best model, but not bad."""
    model = Sequential()
    model.add(Dense(256, input_shape=(vector_length,)))
    model.add(Dropout(0.3))
    model.add(Dense(256, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(128, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(128, activation="relu"))
    model.add(Dropout(0.3))
    model.add(Dense(64, activation="relu"))
    model.add(Dropout(0.3))
    # one output neuron with sigmoid activation function, 0 means female, 1 means male
    model.add(Dense(1, activation="sigmoid"))
    # using binary crossentropy as it's male/female classification (binary)
    model.compile(loss="binary_crossentropy", metrics=["accuracy"], optimizer="adam")
    return model

def extract_feature(file_name: str, **kwargs):
    """
    Extract feature from audio file `file_name`
        Features supported:
            - MFCC (mfcc)
            - Chroma (chroma)
            - MEL Spectrogram Frequency (mel)
            - Contrast (contrast)
            - Tonnetz (tonnetz)
        e.g:
        `features = extract_feature(path, mel=True, mfcc=True)`
    """
    mfcc = kwargs.get("mfcc")
    chroma = kwargs.get("chroma")
    mel = kwargs.get("mel")
    contrast = kwargs.get("contrast")
    tonnetz = kwargs.get("tonnetz")
    start = kwargs.get("start")
    duration = kwargs.get("duration")

    X, sample_rate = librosa.core.load(file_name, offset=start, duration=duration)
    if chroma or contrast:
        stft = np.abs(librosa.stft(X))
    result = np.array([])
    if mfcc:
        mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
        result = np.hstack((result, mfccs))
    if chroma:
        chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)
        result = np.hstack((result, chroma))
    if mel:
        mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)
        result = np.hstack((result, mel))
    if contrast:
        contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)
        result = np.hstack((result, contrast))
    if tonnetz:
        tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=sample_rate).T,axis=0)
        result = np.hstack((result, tonnetz))
    return result

def get_gender(file: str, start: float, duration: float) -> str:
    """
    Fuction for return gender
    Params:
        file: path to wav audio file
        start: wait start secons before start read
        duration: read duration seconds
    """
    model = create_model()
    # load the saved/trained weights
    model.load_weights("model.h5")
    # extract features and reshape it
    features = extract_feature(file, mel=True, start=start, duration=duration).reshape(1, -1)
    # predict the gender!
    male_prob = model.predict(features)[0][0]
    female_prob = 1 - male_prob
    gender = "male" if male_prob > female_prob else "female"
    return gender


def detect_raised_voice(audio_path, start, duration, threshold_db=-20):
    """
    Detects whether the voice in the audio file is raised.

    Parameters:
        audio_path (str): Path to the audio file.
        threshold_db (float): Threshold in decibels to classify raised voice.
                              The higher the value, the more sensitive the detection.
                              Default is -20 dB.

    Returns:
        bool: True if the voice is raised, False otherwise.
    """
    # Load the audio file
    y, sr = librosa.load(audio_path, sr=None, offset=start, duration=duration)
    
    # Calculate the Root Mean Square (RMS) energy
    rms = librosa.feature.rms(y=y)[0]
    
    # Convert RMS to decibels (dB)
    rms_db = librosa.amplitude_to_db(rms, ref=np.max)
    
    # Calculate the average loudness in dB
    avg_loudness_db = np.mean(rms_db)
    
    # Check if the average loudness exceeds the threshold
    is_raised = bool(avg_loudness_db > threshold_db)
    
    return is_raised

# I want use this model, but my laptop can't load this model. Sorry..
# model = Model(model_path="./vosk-model-ru-0.42")
model = Model(lang="ru")
def get_dialog_as_text(file: UploadFile) -> list[dict]:
    # Model for get text from audio
    #Convert mp3 Ð² wav
    audio = AudioSegment.from_file(file.file, format="mp3")
    temp_wav = "temp.wav"
    audio.export(temp_wav, format="wav")

    wf = wave.open(temp_wav, "rb")
    rec = KaldiRecognizer(model, wf.getframerate())
    rec.SetWords(True)

    dialog = []
    total_duration = {"receiver": 0, "transmitter": 0}
    current_speaker = "receiver"

    while True:
        data = wf.readframes(10000)
        if len(data) == 0:
            break
        if rec.AcceptWaveform(data):
            result = json.loads(rec.Result())
            if "text" in result and result["text"]:
                duration = 0
                for res in result["result"]:
                    duration += res['end'] - res['start'] 
                gender = get_gender(temp_wav, result["result"][0]['start'], duration)
                raised_voice = detect_raised_voice(temp_wav, result["result"][0]['start'], duration)
                dialog.append({
                    "source": current_speaker,
                    "text": result["text"],
                    "duration": duration,
                    "gender": gender,
                    "raised_voise": raised_voice,
                })
                total_duration[current_speaker] += duration

                current_speaker = "transmitter" if current_speaker == "receiver" else "receiver"

    os.remove(temp_wav)

    return {
        "dialog": dialog,
        "total_duration": total_duration,
    }
